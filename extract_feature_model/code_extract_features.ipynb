{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09d2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Running on GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Starting extraction for ALL 5 modes...\n",
      "Input: D:\\Study\\7-SP26\\DATxSLP\\Data_after_cut\\test_output\n",
      "Output: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\n",
      "-> Created/Checked folder: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\\Only MFCC\n",
      "-> Created/Checked folder: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\\Only MFBE\n",
      "-> Created/Checked folder: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\\Only Pitch\n",
      "-> Created/Checked folder: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\\MFCC + Pitch\n",
      "-> Created/Checked folder: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\\MFBE + Pitch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [07:40<00:00, 23.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! Please check the folders in: D:\\Study\\7-SP26\\DATxSLP\\Data_after_extract_feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. GPU ENABLED EXTRACTOR\n",
    "# ==============================================================================\n",
    "class HandcraftedFeatureExtractor:\n",
    "    def __init__(self, device, sample_rate=16000, n_mfcc=40, n_mels=80, n_fft=400, hop_length=160):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "        self.device = device \n",
    "\n",
    "        # --- Move transforms to GPU immediately upon initialization ---\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate, n_mfcc=n_mfcc,\n",
    "            melkwargs={\"n_fft\": n_fft, \"n_mels\": n_mels, \"hop_length\": hop_length, \"center\": False}\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mels, \n",
    "            hop_length=hop_length, center=False\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _get_pitch_cpu(self, waveform_cpu):\n",
    "        # This function runs on CPU because Librosa does not support GPU directly\n",
    "        wav_numpy = waveform_cpu.squeeze().numpy()\n",
    "        \n",
    "        try:\n",
    "            # Using pyin to ensure quality\n",
    "            f0, _, _ = librosa.pyin(\n",
    "                wav_numpy, \n",
    "                fmin=60, fmax=500, sr=self.sample_rate, \n",
    "                hop_length=self.hop_length, frame_length=self.n_fft, center=False\n",
    "            )\n",
    "        except:\n",
    "            f0 = np.zeros(1)\n",
    "\n",
    "        f0 = np.nan_to_num(f0)\n",
    "        # Return Tensor (keep on CPU for now)\n",
    "        return torch.from_numpy(f0).view(1, 1, -1).float()\n",
    "\n",
    "    def _align_length(self, feat1, feat2):\n",
    "        min_len = min(feat1.shape[-1], feat2.shape[-1])\n",
    "        return feat1[..., :min_len], feat2[..., :min_len]\n",
    "\n",
    "    def _apply_cmvn(self, feature):\n",
    "        if feature is not None:\n",
    "            mean = feature.mean(dim=-1, keepdim=True)\n",
    "            std = feature.std(dim=-1, keepdim=True)\n",
    "            feature = (feature - mean) / (std + 1e-6)\n",
    "        return feature\n",
    "\n",
    "    def extract_all(self, waveform_cpu):\n",
    "        \"\"\"\n",
    "        Extracts ALL 5 modes at once to save computation time.\n",
    "        Returns a dictionary: { 'ModeName': Tensor, ... }\n",
    "        \"\"\"\n",
    "        output_dict = {}\n",
    "\n",
    "        # 1. GPU Processing (MFCC / MFBE)\n",
    "        waveform_gpu = waveform_cpu.to(self.device)\n",
    "        if waveform_gpu.dim() == 1: waveform_gpu = waveform_gpu.unsqueeze(0)\n",
    "\n",
    "        # Compute base features once\n",
    "        mfcc_base = self.mfcc_transform(waveform_gpu)\n",
    "        mel_base = self.mel_transform(waveform_gpu)\n",
    "        mfbe_base = torch.log(mel_base + 1e-6)\n",
    "\n",
    "        # 2. CPU Processing (Pitch)\n",
    "        # Use the CPU copy of waveform for Librosa\n",
    "        pitch_cpu = self._get_pitch_cpu(waveform_cpu)\n",
    "        pitch_base = pitch_cpu.to(self.device) # Move to GPU for merging\n",
    "\n",
    "        # Ensure pitch dim matches for concatenation\n",
    "        if mfcc_base.dim() == 2: mfcc_base = mfcc_base.unsqueeze(0)\n",
    "        if mfbe_base.dim() == 2: mfbe_base = mfbe_base.unsqueeze(0)\n",
    "\n",
    "        # --- MODE 1: Only MFCC ---\n",
    "        output_dict[\"Only MFCC\"] = self._apply_cmvn(mfcc_base)\n",
    "\n",
    "        # --- MODE 2: Only MFBE ---\n",
    "        output_dict[\"Only MFBE\"] = self._apply_cmvn(mfbe_base)\n",
    "\n",
    "        # --- MODE 3: Only Pitch ---\n",
    "        # Note: We squeeze dim 0 for pitch if it's standalone, depending on desired shape.\n",
    "        # Keeping consistent (1, Time) or (1, 1, Time) -> Let's keep (1, Time) for standalone 1D feature\n",
    "        output_dict[\"Only Pitch\"] = self._apply_cmvn(pitch_base.squeeze(0))\n",
    "\n",
    "        # --- MODE 4: MFCC + Pitch ---\n",
    "        # Align lengths\n",
    "        mfcc_aligned, pitch_aligned_1 = self._align_length(mfcc_base, pitch_base)\n",
    "        combined_mfcc_pitch = torch.cat([mfcc_aligned, pitch_aligned_1], dim=1)\n",
    "        output_dict[\"MFCC + Pitch\"] = self._apply_cmvn(combined_mfcc_pitch)\n",
    "\n",
    "        # --- MODE 5: MFBE + Pitch ---\n",
    "        # Align lengths\n",
    "        mfbe_aligned, pitch_aligned_2 = self._align_length(mfbe_base, pitch_base)\n",
    "        combined_mfbe_pitch = torch.cat([mfbe_aligned, pitch_aligned_2], dim=1)\n",
    "        output_dict[\"MFBE + Pitch\"] = self._apply_cmvn(combined_mfbe_pitch)\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATASET\n",
    "# ==============================================================================\n",
    "class AudioFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, extractor, sample_rate=16000):\n",
    "        self.root_dir = root_dir\n",
    "        self.extractor = extractor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.file_list = []\n",
    "        \n",
    "        # Walk through directories\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.wav', '.flac', '.mp3')):\n",
    "                    self.file_list.append(os.path.join(root, file))\n",
    "\n",
    "    def __len__(self): return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_list[idx]\n",
    "        try:\n",
    "            # Read audio\n",
    "            wav_numpy, sr = sf.read(path)\n",
    "            waveform = torch.from_numpy(wav_numpy).float()\n",
    "            \n",
    "            # Basic preprocessing\n",
    "            if waveform.dim() == 1: waveform = waveform.unsqueeze(0)\n",
    "            else: waveform = waveform.t()\n",
    "            if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if sr != self.sample_rate: waveform = T.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "            # Extract ALL features\n",
    "            # Returns a dict of tensors on GPU\n",
    "            features_dict_gpu = self.extractor.extract_all(waveform)\n",
    "            \n",
    "            # Move all tensors to CPU to prevent GPU OOM during DataLoader batching\n",
    "            features_dict_cpu = {k: v.cpu().squeeze(0) for k, v in features_dict_gpu.items()}\n",
    "            \n",
    "            return features_dict_cpu, path \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function to handle dictionary of features.\n",
    "    \"\"\"\n",
    "    # Filter failed samples\n",
    "    batch = [b for b in batch if b[0] is not None]\n",
    "    if not batch: return None\n",
    "\n",
    "    # batch is a list of tuples: (feature_dict, path)\n",
    "    list_of_dicts, paths = zip(*batch)\n",
    "    \n",
    "    # Initialize output dictionary\n",
    "    batched_output = {}\n",
    "    \n",
    "    # Get all mode keys from the first sample\n",
    "    keys = list_of_dicts[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        # Collect all tensors for this specific mode\n",
    "        tensors = [d[key] for d in list_of_dicts]\n",
    "        \n",
    "        # Pad them\n",
    "        max_len = max([t.shape[-1] for t in tensors])\n",
    "        padded_tensors = [torch.nn.functional.pad(t, (0, max_len - t.shape[-1])) for t in tensors]\n",
    "        \n",
    "        # Stack them into a batch\n",
    "        batched_output[key] = torch.stack(padded_tensors)\n",
    "        \n",
    "    return batched_output, paths\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURATION ---\n",
    "    INPUT_PATH = r\"E:\\speech_data\\train_vi_7s\"\n",
    "    OUTPUT_BASE_PATH = r\"E:\\speech_data\\7s_extracted_features\"\n",
    "    \n",
    "    # List of modes to verify logic (The script extracts ALL of these automatically)\n",
    "    MODES_TO_SAVE = [\"Only MFCC\", \"Only MFBE\", \"Only Pitch\", \"MFCC + Pitch\", \"MFBE + Pitch\"]\n",
    "    \n",
    "    # --- SETUP GPU ---\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\" Running on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\" GPU not found, running on CPU.\")\n",
    "\n",
    "    # Initialize Extractor\n",
    "    extractor = HandcraftedFeatureExtractor(device=device)\n",
    "    \n",
    "    # Initialize Dataset (No mode needed here anymore)\n",
    "    dataset = AudioFolderDataset(INPUT_PATH, extractor)\n",
    "    \n",
    "    # Note: On Windows, num_workers should be 0 to avoid multiprocessing issues with CUDA\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"Starting extraction for ALL {len(MODES_TO_SAVE)} modes...\")\n",
    "    print(f\"Input: {INPUT_PATH}\")\n",
    "    print(f\"Output: {OUTPUT_BASE_PATH}\")\n",
    "\n",
    "    # Create sub-folders for each mode\n",
    "    for mode in MODES_TO_SAVE:\n",
    "        mode_folder = os.path.join(OUTPUT_BASE_PATH, mode)\n",
    "        os.makedirs(mode_folder, exist_ok=True)\n",
    "        print(f\"-> Created/Checked folder: {mode_folder}\")\n",
    "\n",
    "    # Process loop\n",
    "    for batch in tqdm(loader):\n",
    "        if batch is None: continue\n",
    "        \n",
    "        # batched_features is a dict: {'Only MFCC': TensorBatch, ...}\n",
    "        batched_features, paths = batch\n",
    "        \n",
    "        # Iterate over each mode and save\n",
    "        for mode, features_tensor in batched_features.items():\n",
    "            \n",
    "            # Define output folder for this mode\n",
    "            mode_output_dir = os.path.join(OUTPUT_BASE_PATH, mode)\n",
    "            \n",
    "            for i in range(len(paths)):\n",
    "                # Calculate relative path to maintain folder structure (SpeakerID/File)\n",
    "                rel_path = os.path.relpath(paths[i], INPUT_PATH)\n",
    "                \n",
    "                # Construct save path: OutputBase/ModeName/SpeakerID/Filename.pt\n",
    "                save_path = os.path.join(mode_output_dir, os.path.splitext(rel_path)[0] + \".pt\")\n",
    "                \n",
    "                # Create parent directory (SpeakerID folder) inside the Mode folder\n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                \n",
    "                # Save the tensor\n",
    "                # Clone is used to detach from the batch tensor to save memory\n",
    "                torch.save(features_tensor[i].clone(), save_path)\n",
    "\n",
    "    print(f\"\\nCompleted! Please check the folders in: {OUTPUT_BASE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
