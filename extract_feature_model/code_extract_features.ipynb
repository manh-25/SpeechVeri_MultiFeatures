{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c09d2977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Running on GPU: NVIDIA GeForce RTX 5080\n",
      "-> Scanning input directory: D:\\Speech_Verification\\cut_audio_5s\n",
      "-> Checking output directory for existing files to resume...\n",
      "-> Total found: 394182\n",
      "-> Skipped (Already processed): 480\n",
      "-> Remaining to process: 393702\n",
      "\n",
      "Starting extraction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24607 [00:00<?, ?it/s]C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_15644\\586799837.py:38: UserWarning: With fmin=60.000, sr=16000 and frame_length=400, less than two periods of fmin fit into the frame, which can cause inaccurate pitch detection. Consider increasing to fmin=80.000 or frame_length=535.\n",
      "  f0, _, _ = librosa.pyin(\n",
      " 10%|‚ñâ         | 2432/24607 [6:21:20<59:10:17,  9.61s/it]C:\\Users\\Dung\\AppData\\Local\\Temp\\ipykernel_15644\\586799837.py:56: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\ReduceOps.cpp:1857.)\n",
      "  std = feature.std(dim=-1, keepdim=True)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24607/24607 [62:35:43<00:00,  9.16s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Completed! Check: D:\\Speech_Verification\\cut_audio_5s_features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. GPU ENABLED EXTRACTOR\n",
    "# ==============================================================================\n",
    "class HandcraftedFeatureExtractor:\n",
    "    def __init__(self, device, sample_rate=16000, n_mfcc=40, n_mels=80, n_fft=400, hop_length=160):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.hop_length = hop_length\n",
    "        self.n_fft = n_fft\n",
    "        self.device = device \n",
    "\n",
    "        # --- Move transforms to GPU immediately upon initialization ---\n",
    "        self.mfcc_transform = T.MFCC(\n",
    "            sample_rate=sample_rate, n_mfcc=n_mfcc,\n",
    "            melkwargs={\"n_fft\": n_fft, \"n_mels\": n_mels, \"hop_length\": hop_length, \"center\": False}\n",
    "        ).to(self.device)\n",
    "\n",
    "        self.mel_transform = T.MelSpectrogram(\n",
    "            sample_rate=sample_rate, n_fft=n_fft, n_mels=n_mels, \n",
    "            hop_length=hop_length, center=False\n",
    "        ).to(self.device)\n",
    "\n",
    "    def _get_pitch_cpu(self, waveform_cpu):\n",
    "        # This function runs on CPU because Librosa does not support GPU directly\n",
    "        wav_numpy = waveform_cpu.squeeze().numpy()\n",
    "        \n",
    "        try:\n",
    "            # Using pyin to ensure quality\n",
    "            f0, _, _ = librosa.pyin(\n",
    "                wav_numpy, \n",
    "                fmin=60, fmax=500, sr=self.sample_rate, \n",
    "                hop_length=self.hop_length, frame_length=self.n_fft, center=False\n",
    "            )\n",
    "        except:\n",
    "            f0 = np.zeros(1)\n",
    "\n",
    "        f0 = np.nan_to_num(f0)\n",
    "        return torch.from_numpy(f0).view(1, 1, -1).float()\n",
    "\n",
    "    def _align_length(self, feat1, feat2):\n",
    "        min_len = min(feat1.shape[-1], feat2.shape[-1])\n",
    "        return feat1[..., :min_len], feat2[..., :min_len]\n",
    "\n",
    "    def _apply_cmvn(self, feature):\n",
    "        if feature is not None:\n",
    "            mean = feature.mean(dim=-1, keepdim=True)\n",
    "            std = feature.std(dim=-1, keepdim=True)\n",
    "            feature = (feature - mean) / (std + 1e-6)\n",
    "        return feature\n",
    "\n",
    "    def extract_all(self, waveform_cpu):\n",
    "        \"\"\"\n",
    "        Extracts ALL 5 modes at once.\n",
    "        Returns a dictionary: { 'ModeName': Tensor, ... }\n",
    "        \"\"\"\n",
    "        output_dict = {}\n",
    "\n",
    "        # 1. GPU Processing\n",
    "        waveform_gpu = waveform_cpu.to(self.device)\n",
    "        if waveform_gpu.dim() == 1: waveform_gpu = waveform_gpu.unsqueeze(0)\n",
    "\n",
    "        mfcc_base = self.mfcc_transform(waveform_gpu)\n",
    "        mel_base = self.mel_transform(waveform_gpu)\n",
    "        mfbe_base = torch.log(mel_base + 1e-6)\n",
    "\n",
    "        # 2. CPU Processing (Pitch)\n",
    "        pitch_cpu = self._get_pitch_cpu(waveform_cpu)\n",
    "        pitch_base = pitch_cpu.to(self.device) \n",
    "\n",
    "        if mfcc_base.dim() == 2: mfcc_base = mfcc_base.unsqueeze(0)\n",
    "        if mfbe_base.dim() == 2: mfbe_base = mfbe_base.unsqueeze(0)\n",
    "\n",
    "        # Combine Features\n",
    "        output_dict[\"Only MFCC\"] = self._apply_cmvn(mfcc_base)\n",
    "        output_dict[\"Only MFBE\"] = self._apply_cmvn(mfbe_base)\n",
    "        output_dict[\"Only Pitch\"] = self._apply_cmvn(pitch_base.squeeze(0))\n",
    "\n",
    "        mfcc_aligned, pitch_aligned_1 = self._align_length(mfcc_base, pitch_base)\n",
    "        output_dict[\"MFCC + Pitch\"] = self._apply_cmvn(torch.cat([mfcc_aligned, pitch_aligned_1], dim=1))\n",
    "\n",
    "        mfbe_aligned, pitch_aligned_2 = self._align_length(mfbe_base, pitch_base)\n",
    "        output_dict[\"MFBE + Pitch\"] = self._apply_cmvn(torch.cat([mfbe_aligned, pitch_aligned_2], dim=1))\n",
    "\n",
    "        return output_dict\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. DATASET (UPDATED WITH RESUME LOGIC)\n",
    "# ==============================================================================\n",
    "class AudioFolderDataset(Dataset):\n",
    "    def __init__(self, root_dir, output_base_dir, modes_list, extractor, sample_rate=16000):\n",
    "        self.root_dir = root_dir\n",
    "        self.extractor = extractor\n",
    "        self.sample_rate = sample_rate\n",
    "        self.file_list = []\n",
    "        \n",
    "        print(f\"-> Scanning input directory: {root_dir}\")\n",
    "        print(f\"-> Checking output directory for existing files to resume...\")\n",
    "\n",
    "        total_files = 0\n",
    "        skipped_files = 0\n",
    "        \n",
    "        # Walk through directories\n",
    "        for root, _, files in os.walk(root_dir):\n",
    "            for file in files:\n",
    "                if file.lower().endswith(('.wav', '.flac', '.mp3')):\n",
    "                    total_files += 1\n",
    "                    input_path = os.path.join(root, file)\n",
    "                    \n",
    "                    # --- RESUME LOGIC ---\n",
    "                    # 1. Calculate the relative path (e.g., \"Speaker1/audio01.wav\")\n",
    "                    rel_path = os.path.relpath(input_path, root_dir)\n",
    "                    # 2. Change extension to .pt (e.g., \"Speaker1/audio01.pt\")\n",
    "                    rel_path_pt = os.path.splitext(rel_path)[0] + \".pt\"\n",
    "                    \n",
    "                    # 3. Check if THIS file exists in ALL 5 output mode folders\n",
    "                    all_outputs_exist = True\n",
    "                    for mode in modes_list:\n",
    "                        expected_output_path = os.path.join(output_base_dir, mode, rel_path_pt)\n",
    "                        if not os.path.exists(expected_output_path):\n",
    "                            all_outputs_exist = False\n",
    "                            break\n",
    "                    \n",
    "                    # 4. If all exist, skip. If any is missing, process it.\n",
    "                    if all_outputs_exist:\n",
    "                        skipped_files += 1\n",
    "                    else:\n",
    "                        self.file_list.append(input_path)\n",
    "\n",
    "        print(f\"-> Total found: {total_files}\")\n",
    "        print(f\"-> Skipped (Already processed): {skipped_files}\")\n",
    "        print(f\"-> Remaining to process: {len(self.file_list)}\")\n",
    "\n",
    "    def __len__(self): return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.file_list[idx]\n",
    "        try:\n",
    "            wav_numpy, sr = sf.read(path)\n",
    "            waveform = torch.from_numpy(wav_numpy).float()\n",
    "            \n",
    "            if waveform.dim() == 1: waveform = waveform.unsqueeze(0)\n",
    "            else: waveform = waveform.t()\n",
    "            if waveform.shape[0] > 1: waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "            if sr != self.sample_rate: waveform = T.Resample(sr, self.sample_rate)(waveform)\n",
    "\n",
    "            features_dict_gpu = self.extractor.extract_all(waveform)\n",
    "            features_dict_cpu = {k: v.cpu().squeeze(0) for k, v in features_dict_gpu.items()}\n",
    "            \n",
    "            return features_dict_cpu, path \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = [b for b in batch if b[0] is not None]\n",
    "    if not batch: return None\n",
    "    list_of_dicts, paths = zip(*batch)\n",
    "    batched_output = {}\n",
    "    keys = list_of_dicts[0].keys()\n",
    "\n",
    "    for key in keys:\n",
    "        tensors = [d[key] for d in list_of_dicts]\n",
    "        max_len = max([t.shape[-1] for t in tensors])\n",
    "        padded_tensors = [torch.nn.functional.pad(t, (0, max_len - t.shape[-1])) for t in tensors]\n",
    "        batched_output[key] = torch.stack(padded_tensors)\n",
    "        \n",
    "    return batched_output, paths\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. EXECUTION\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # --- CONFIGURATION ---\n",
    "    INPUT_PATH = r\"D:\\Speech_Verification\\cut_audio_5s\"\n",
    "    OUTPUT_BASE_PATH = r\"D:\\Speech_Verification\\cut_audio_5s_features\"\n",
    "    \n",
    "    # Must list ALL modes here to check for completion\n",
    "    MODES_TO_SAVE = [\"Only MFCC\", \"Only MFBE\", \"Only Pitch\", \"MFCC + Pitch\", \"MFBE + Pitch\"]\n",
    "    \n",
    "    # --- SETUP GPU ---\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"üî• Running on GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ö†Ô∏è GPU not found, running on CPU.\")\n",
    "\n",
    "    extractor = HandcraftedFeatureExtractor(device=device)\n",
    "    \n",
    "    # Pass output path and modes to Dataset for Resume Logic\n",
    "    dataset = AudioFolderDataset(\n",
    "        root_dir=INPUT_PATH, \n",
    "        output_base_dir=OUTPUT_BASE_PATH, \n",
    "        modes_list=MODES_TO_SAVE, \n",
    "        extractor=extractor\n",
    "    )\n",
    "    \n",
    "    # If everything is processed, exit early\n",
    "    if len(dataset) == 0:\n",
    "        print(\"\\n‚úÖ All files have been processed. Nothing to do!\")\n",
    "        exit()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "    print(f\"\\nStarting extraction...\")\n",
    "    \n",
    "    # Create sub-folders (idempotent)\n",
    "    for mode in MODES_TO_SAVE:\n",
    "        os.makedirs(os.path.join(OUTPUT_BASE_PATH, mode), exist_ok=True)\n",
    "\n",
    "    # Process loop\n",
    "    for batch in tqdm(loader):\n",
    "        if batch is None: continue\n",
    "        \n",
    "        batched_features, paths = batch\n",
    "        \n",
    "        for mode, features_tensor in batched_features.items():\n",
    "            mode_output_dir = os.path.join(OUTPUT_BASE_PATH, mode)\n",
    "            \n",
    "            for i in range(len(paths)):\n",
    "                rel_path = os.path.relpath(paths[i], INPUT_PATH)\n",
    "                save_path = os.path.join(mode_output_dir, os.path.splitext(rel_path)[0] + \".pt\")\n",
    "                \n",
    "                os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "                torch.save(features_tensor[i].clone(), save_path)\n",
    "\n",
    "    print(f\"\\nCompleted! Check: {OUTPUT_BASE_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
